
\chapter{Chapter 2 - Overview of supervised learning}

\subsection*{2.1 Suppose each of K-classes has an associated target $t_k$ which is a vector of all zeros except one in the $k$th position. Show that classifying to the largest element of $\hat{y}$ amounts to choosing the closest target $min_k \norm{t_k - \hat{y}}$ if the elements of $y$ sum to one.}

Our norm here is the $L^2$ norm. 
\begin{align*}
    argmin_k \norm{t_k - \hat{y}} &=  argmin_k \norm{t_k - \hat{y}}^2 \\
    &= argmin_k \sum_i \left(\hat{y}_i - (t_k)_i\right)^2 \\
    &= argmin_k \sum_i \left(\hat{y}_i - \delta_{i, k}\right)^2 \\
    &= argmin_k \left( 1 - 2 \hat{y}_k + \sum_i  \hat{y}_i^2\right)  \\
    &= argmin_k \left(-2\hat{y}_k\right)
\end{align*}
Where the last line follows as it is the only term dependant on $k$. $argmin$ is independent of scale, so the above states that the $k$ corresponding to the minimum value of the norm is exactly the largest element of $\hat{y}$


\subsection*{2.2 Show how to compute the Bayes decision boundary for the example in Figure 2.5.}
Setup: There are 10 means $m_k$ from a bivariate guassian distribution $N((1,0)^T, I)$ and we label this class blue.
We take 10 $n_k$ from $N((0,1)^T, I)$ and label this class orange.
For each class there were 100 observations, where each observation was generated by picking one of the $m_k$ ($n_k$ resp) each with equal probability, and taking a point randomly from a $N(m_k, I/5)$ distribution.

Let $x$ be an observation. We equate posteriors for $x$ being blue or yellow, and note that in our setup $\mathbb{P}(blue) = \mathbb{P}(orange)$ (priors are equal) to simplify to:
$$\sum_k exp(-5 \norm{m_k - x}^2) = \sum_k exp(-5 \norm{n_k - x}^2)$$
This defines a curve in the plane separating the two classes.

\subsection*{2.3 Derive equation 2.24.}
Setup: Consider $N$ data points uniformly distributed in a $p$-dimensional unit ball around the origin. Consider a nearest neighbour estimate at the origin. The median distance from the origin to the closest point is given by:
$$ d(p_N) = \left(1 - \frac{1}{2}^{1/N}\right)^{1/p} $$

Solution: Let $x$ be a point and let $y = \norm{x}$.
$y$ has cdf equal to the ratio of a ball of radius $y$ to a ball of radius $1$, i.e. $F(y) = y^p$. The minimum over all $x$ then has cdf $F_{ymin}(y) = 1 - \left(1 - F(y)\right)^N$ (a general fact for order statistics). 
Thus $F_{ymin} = 1 - \left(1 - y^p\right)^N$.

The median distance for $ymin$ is when $F_{ymin}(y) = 1/2$. Solving for this yields the result.

\subsection*{2.4 Setup as in book. Projection in direction $a$.}

Pick an orthonormal basis of $\mathbb{R}^p$ which includes the vector $a$, say $a_1, \dots, a_p$
with $a_1 = a$. Then each $x_i = \sum_j X_{i,j} a_j$, and so $z_i = X_{i,1}$ where $X$ is the matrix with rows $x_i$

The $x_i$ have distribution $N(0, I_p)$, and under such a distribution each component of $x_i$ has distribution $N(0,1)$.

In particular this means that each $X_{i,j}$ has distribution $N(0,1)$ and so the $z_i$ do.

The squared distance from the origin is just $z_i^2$, with distribution $\chi^2_1$, and this has mean 1.


\subsection*{2.6 Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameters model $f_\theta(x)$ to be fit by least squares. Show that if there are observations with tied or identical values of $x$ then the fir can be obtained from a reduced weighted least squares problem.}

The problem can of finding $\theta$ amounts to solving the following:
$$argmin_\theta \left(y - f_\theta(x)\right)^T \left(y - f_\theta(x)\right)$$

Denote by $z_1, \dots, z_M$ the unique values of $x$ in our training set, denote by $n_j$ the number of occurrences of value $z_j$. 
Then let $t_j = \frac{1}{n_j}\sum_{i=1}^{n_j} y_i$
If we can get to the following, we're done (as it is in the form of weighted regression). 
$$argmin_\theta \sum_j n_j \left(t_j - f_\theta(z_j)\right)^2$$

Expanding the initial expression we get (denoting by $y_{i,j}$ the $i$th value of $y$ corresponding to input $z_j$):
\begin{align*}
     \left(y - f_\theta(x)\right)^T \left(y - f_\theta(x)\right) &= \sum_i \left(y_i - f_\theta(x_i)\right)^2 \\
    &= \sum_i \left(y_i^2 + f_\theta(x_i)^2 - 2 y_i f_\theta(x_i)\right) \\
    &= \sum_i \left(y_i^2 + f_\theta(x_i)^2 - 2 y_i f_\theta(x_i)\right) \\
    &= \sum_{j=1}^M \sum_{i=1}^{n_j}  y_{i,j}^2 - 2 f_\theta(z_j) y_{i,j} + f_\theta(z_j)^2\\
    &= \sum_{j=1}^M   \left(\sum_i y_{i,j}^2\right) - 2 n_j f_\theta(z_j) t_j + n_j f_\theta(z_j)^2\\
    &= \sum_{j=1}^M   n_j  (t_j -  f_\theta(z_j)) ^ 2 - \sum_{j=1}^M n_j t_j^2 + \sum_{j=1}^M   \left(\sum_i y_{i,j}^2\right) 
\end{align*}
This last trick of adding $0$ leaves us with an expression where only the first sum is dependant on $\theta$
Thus when taking $argmin_\theta$ we can ignore the last two terms.

This leaves us with the required equivalence.:

$$argmin_\theta \left(y - f_\theta(x)\right)^T \left(y - f_\theta(x)\right) = argmin_\theta \sum_j n_j \left(t_j - f_\theta(z_j)\right)^2$$

