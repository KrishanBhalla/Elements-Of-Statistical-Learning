

\chapter{Chapter 3 - Linear methods for Regression}

\subsection*{3.1 Show that the F-statistic for dropping a single coefficient of a model is equivalent to the square of the corresponding z score}

Let $X$ be our data, and let $v_{j,j}$ be the $j$th diagonal element of $V = (X^T X)^{-1}$. $z_j = \hat{\beta}_j / \hat{\sigma} \sqrt{v_{j,j}}$ is the z score. 

The F statistic is $$ F = \frac{(RSS_0 - RSS_1) / (p1 - p0)}{RSS_1 / (N - p_1 - 1)}$$
Where the regression models are have $p_1 + 1$ and $p_0 + 1$ degrees of freedom respectively.
We also know that $\hat{\sigma}^2$ is equivalent to the denominator. In the case of dropping a single variable, this simplifies to:
$$ F = \frac{RSS_0 - RSS_1}{\hat{\sigma}^2}$$

Where $\hat{\sigma}$ is derived from the bigger model.

Thus our question can be simplifed to showing that: 

$$ RSS_0 - RSS_1 = \hat{\beta}_n^2 /  v_{j,j} $$

We know $\hat{\beta} ~ N(\beta, \sigma^2 V)$ and so $\hat{\beta}_j ~ N(\beta_j, \sigma^2 v_{j,j})$.

Under the null-hypothesis $\beta_j = 0$ and so: $\hat{\beta}_j = \sigma \sqrt{v_{j,j}} Z$ where $Z \sim N(0,1)$
Thus $\hat{\beta}^2_j = \sigma^2 v_{j,j} Z^2 = \sigma^2 v_{j,j} Q$ where $Q \sim \chi^2_1$

Similarly $RSS_0, RSS_1$ have distribution $\sigma^2 \chi^2_{N_i}$ where $N_i$ is the number of degrees of freedom. Thus $RSS_0 - RSS_1  \sim \sigma^2 \chi^2_1$

Hence $\hat{\beta}_j / v_{j,j}$ and $RSS_0 - RSS_1$ have the same distribution. They further test the same hypothesis and thus must be identical.








\subsection*{3.3 
a. Prove the Gauss-Markov theorem: the least squares estimate of a parameter $a^T\beta$ has a variance no bigger than that of any other linear unbiased estimate of $a^T\beta$.\\
b. Secondly, show that if $\hat{V}$ is the variance-covariance matrix of the least squares estimate of $\beta$ and $\tilde{V}$ is the variance covariance matrix of any other linear unbiased estimate, then $\hat{V} \leq \tilde{V}$, where $B \leq A$ if $A - B$ is positive semidefinite.}

First note that part b implies part a. If $\beta$ has dimension 1, then $V$ is just the variance of beta, and $\leq$ is equivalent to the normal $\leq$ operator. Taking the inner product with $a$ is just a linear operation. We thus only need to show b.


Suppose $\hat{\beta}$ is the OLS estimate of $\beta$ and that $\tilde{\beta}$ is another linear unbiased estimate. The the variance-covariance matrices be $\hat{V}$ and $\tilde{V}$ resp.
$\hat{\beta} = \left(X^T X\right)^{-1} X^T y$ so $\hat{\beta} = C y$ say, and write $\tilde{\beta} = (C + D) y $ for some non-zero $m x n$ matrix $D$.
\begin{align*}
    \mathbb{E}\left(\tilde{\beta}\right) &= \mathbb{E}\left(\left(C + D\right) \left(X\beta + \epsilon\right) \right) \\
    &= \mathbb{E} \left(\left(X^T X\right)^{-1} X^T + D\right) \left(X\beta + \epsilon\right)  \\
    &= \mathbb{E}\left(\left(\beta + DX\beta \right) \right) + \zeta \mathbb{E}(\epsilon) \\
    &= \mathbb{E}\left(\left(\beta + DX\beta \right) \right) \\
    &= \beta + DX\beta \\
    &= \beta
\end{align*}
Where the last lines follow as $\tilde{\beta}$ is unbiased. In particular $DX\beta = 0$ and so $DX = 0$ as beta is an unobserved parameter to be estimated.

\begin{align*}
    \tilde{V} &= Var\left(\tilde{\beta}\right)\\
    &= Var\left(\left(C + D\right) y \right) \\
    &= \left(C + D\right) \left(C + D\right)^T Var\left( y \right) \\
    &= \sigma^2 \left(C + D\right) \left(C + D\right)^T\\
    &= \sigma^2 \left( C C^T + C D^T  + D C^T + D D^T\right)\\
    &= \sigma^2 \left(C C^T  + C D^T  + D C^T + D D^T\right)\\
    &= \hat{V} + \sigma^2 \left(+ C D^T  + D C^T + D D^T\right) \\
    &= \hat{V} + \sigma^2 \left(\left(X^T X\right)^{-1}X^T D^T  +   D X \left(X^T X\right)^{-1} + D^T D\right) \\
    &= \hat{V} + \sigma^2 D D^T
\end{align*}
Where we know that $D D^T$ is positive semi-definite\footnote{$v^T D D^T v = \norm{D^T v} \geq 0$ $ \forall v $} and so are done.


\subsection*{ 3.4 Show how the vector of least squares coefficients can be obtained
from a single pass of the Gramâ€“Schmidt procedure (Algorithm 3.1). Represent your solution in terms of the QR decomposition of X.}

Let $X = QR$ be the $QR$ decomposition of $X$. This can be attained via Gram-Schmidt. We assume that $R$ has no zeros on the diagonal (i.e. the variables are linearly independent).
Then 
\begin{align*}
    \hat{\beta} &= (X^T X)^{-1} X^T y \\
    &= (R^T R)^{-1} R^T Q^T y \\
    &= R^{-1} Q^T y
\end{align*}

We can invert $R$ via backpropogation, and $Q^T y$ is easily calculable.






\subsection*{3.5 Show that the ridge regression problem (using $\alpha$ to denote the constant intercept vector)
$$\hat{\beta} = argmin_{\beta,\alpha} \left(y - \alpha - X \beta\right)^T\left(y - \alpha - X \beta\right) + \lambda \norm{\beta}^2 $$
is equivalent to the problem:
$$\hat{\beta^c} = argmin_{\beta^c, \alpha^c} \left(y - \alpha^c - \tilde{X} \beta^c\right)^T\left(y - \alpha^c - \tilde{X} \beta^c\right) + \lambda \norm{\beta^c}^2 $$
Where $\tilde{X} = X - \bar{X}$, and bar represents the the $N x p)$ matrix where each value in the $j$th column is the mean $x_j$. Give the correspondence between $\beta^c$ and the original $\beta$. Do the same for the lasso.\footnote{
This is all a tad odd as regression with intercept is desired, but we only penalise the non-intercept terms.}}

This problem is easier with summation

\begin{align*}
   &\left(y - \alpha - X \beta\right)^T\left(y - \alpha - X \beta\right) + \lambda \norm{\beta}^2\\ &=
   \left(y - \alpha - \bar{X} \beta - (X - \bar{X}) \beta\right)^T\left(y - \alpha - \bar{X} \beta - (X - \bar{X}) \beta\right) + \lambda \norm{\beta}^2\\ &=
   \left(y - (\alpha + \bar{X} \beta) - (X - \bar{X}) \beta\right)^T\left(y - (\alpha + \bar{X} \beta) - (X - \bar{X}) \beta\right) + \lambda \norm{\beta}^2\\ &=
   \left(y - \alpha^c - \tilde{X} \beta^c\right)^T\left(y - \alpha^c - \tilde{X} \beta^c\right) + \lambda \norm{\beta^c}^2
\end{align*}
Where $\alpha^c_i = \alpha_i + \sum_{j = 1}^p \bar{x}_j \beta_j$ in all coordinates, and $\beta^c = \beta$. This is an expression of our desired form.
This problem is equivalent to demeaning the data and adjusting the intercept.
One can do exactly the same for the lasso.







\subsection*{3.6 Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\beta ~ N(0, \tau I)$ and Gaussian sampling model $y ~ N(X\beta, \sigma^2 I)$. Find the relationship between the ridge parameter $\lambda$ and the variances $\tau$ and $\sigma^2$}

This question really states that the pdf of the posterior is proportional to the pdfs of $y$ given $\beta$ and $\beta$.
Hence:
\begin{align}
    f(\beta\vert y) &\propto f(\beta) f(y\vert \beta) \\
    log(f(\beta\vert y)) &= C + log(f(\beta)) + log(f(y\vert \beta)) \\
     &= C + -\frac{1}{2\tau}\beta^T \beta * log\left(\frac{1}{\sqrt{2*\pi*\tau}}\right)\\ &-\frac{1}{2\sigma^2}\left(y - X\beta\right) ^T \left(y - X\beta\right) *  log\left(\frac{1}{\sqrt{2*\pi*\sigma^2}}\right) \\ 
     &= C + -\frac{1}{2\tau}\beta^T \beta  +  -\frac{1}{2\sigma^2}\left(y - X\beta\right) ^T \left(y - X\beta\right)
\end{align}
Where we absorb terms into the constant as needed. 
We have recovered that: 
\begin{align}
    f(\beta \vert y) &= C_1e^{ -\frac{1}{2\sigma^2}\left(\frac{\sigma^2}{\tau} \beta^T \beta  +\left(y - X\beta\right) ^T \left(y - X\beta\right)\right)} \\
    &= C_1e^{ -\frac{1}{2\sigma^2}\left(\beta^T\left(X^T X + \frac{\sigma^2}{\tau} I \right) \beta + y^T y - y^T X \beta - \beta^T X^T y\right)}\\
    &= C_1e^{ -\frac{1}{2\sigma^2}\left(\beta^T \Sigma  \beta + y^T y - y^T X \beta - \beta^T X^T y\right)}\\
    &= C_1e^{ -\frac{1}{2\sigma^2}\left(\left(\Sigma\beta - X^T y\right)^T\Sigma^{-1}   \left(\Sigma\beta - X^T y\right) + y^T y - y^T X^T X y\right)}\\
    &= C_2e^{ -\frac{1}{2\sigma^2}\left(\Sigma\beta - X^T y\right)^T\Sigma^{-1}   \left(\Sigma\beta - X^T y\right)}\\
\end{align}

Where $\Sigma = \left(X^T X + \frac{\sigma^2}{\tau} I \right)$ is a $(p+1) * (p+1)$ matrix, and for (11) we absorbed into the constant any terms not dependant on $\beta$
 
Thus the posterior has a multivariate gaussian pdf (up to scaling), so the mean and mode of the distribution are identical, and can be found by maximising (5) over $\beta$, which due to the minus sign is sufficient.

This amounts to
$$argmin_\beta \left( \frac{1}{\tau}\beta^T \beta  +  \frac{1}{\sigma^2}\left(y - X\beta\right) ^T \left(y - X\beta\right)\right)$$ 
And hence is equivalent to solving:
$$argmin_\beta  \left(\frac{\sigma^2}{\tau}\beta^T \beta  + \left(y - X\beta\right) ^T \left(y - X\beta\right)\right)$$

Hence the ridge regression parameter is $\sigma^2 / \tau$






\subsection*{3.9 Forward stepwise regression: Suppose we have the QR decomposition for the N x q matrix $X_1$ in a multiple regression problem with response $y$, and suppose we have an additional $p-q$ predictors in the matrix $X_2$. Denote the residual by $r$. Describe an efficient procedure for establishing which additional variable will reduce the residual sum of squares the most.}

Intuition - pick the column $\hat{v}$ such that $v$ has the least angle with $r$.
i.e. $$\hat{v} = argmax_{v \in \{columns X_2\}} \frac{\lvert r^T v\rvert}{\norm{v}}$$

With this in mind, let $u_j = x_j - \frac{\lvert r^T x_j\rvert}{\norm{x_j}}$ where the $x_j$ are the columns of $X_2$, and let $v_j = \frac{u_j}{\norm{u_j}}$

\begin{align*}
    RSS &= r^T r \\
    r &= y - \hat{y} \\
    &= y - R^{-1} Q^T y\\
    let\hspace{1mm} r_j &= r -  \left(r^T u_j\right) u_j \\
    and\hspace{1mm} RSS_j &= RSS - 2 \left(r^T u_j\right)^2  + \left(r^T u_j\right)^2\\
    then\hspace{1mm} RSS_j &= RSS -  \left(r^T u_j\right)^2\\
\end{align*}
Where $RSS_j$ is the residual sum of squares for our new model.
This verifies our intuition, $RSS_j$ is minimised when we pick the column of $X_2$ with least angle to $r$.
I assume the efficiency in the question comes from the ease of inverting $R$ compared to $X^T X$ (backpropogation will do), and that we can extend our $QR$ decomposition to include the new variable easily via Gram-Schmidt. In particular most of what is needed for Gram-Schmidt is already computed when taking inner product with the residual.






\subsection*{3.10 Backward stepwise regression. Suppose we have the multiple regression fit of $y$ on $X$ along with the standard errors and Z-scores. We wish to establish which variable, when dropped, will increase the RSS the least. How would you do this?}

From question 3.1 we know that the F-statistic for dropping a single coefficient of a model is equivalent to the square of the corresponding Z score.
Further, we know the F statistic in the case of dropping 1 variable is:
$$ \frac{\left(RSS_0 - RSS_1\right)}{RSS_1 / (N - p1 - 1)}$$
where $N$, $p_1$ and $RSS_1$ are constant. In particular, the change is RSS is proportional to the F-score with a constant that does not depend on choice of variable to be dropped, and so the change in RSS is proportional to the square of the z-score in the same manner. Thus the difference will be smallest (smallest increase in RSS) if the variable has minimal z-score in our model.
\subsection*{3.12 Show the ridge regression estimates can be obtained by OLS regression on an augmented data set. Add $p$ rows to the centered matrix $X$, $\sqrt{\lambda} I_p$, and augment $y$ with $p$ zeros.}

Let $X_2 = [X^T,\sqrt{\lambda} I_p]^T$ be our augmented matrix, and let $y_2 = [y^T,0]^T$ be the augmented response.
Under OLS, we have $\beta_2 = (X_2^T X_2)^{-1} X_2^T Y$.
\begin{align*}
X_2^T X_2 &= [X^T,\sqrt{\lambda} I_p] [X^T,\sqrt{\lambda} I_p]^T \\
&= X^T X + \lambda I_p\\
X_2^T y_2 &= [X^T,\sqrt{\lambda} I_p] [y^T,0]^T \\
&= X^T y\\
\end{align*}

Thus $\beta = \left(X^T X + \lambda I_p\right)X^T y$ which is exactly the ridge regression beta for our non-augmented dataset.





\subsection*{3.13 Derive expression 3.62 and show that $\hat{\beta}^{pcr}(p) = \hat{\beta}^{ls}$}

Given an SVD of $X$, $X = U D V^T$ say, with $V = [v_1,\dots,v_p]$, the principal components $z_m$ in equation 3.61 are defined as $X v_m$ for all $m$.
The principal component regression is, for any $M <= p$:
\begin{align*}
\hat{y}^{pcr} &= \bar{y} + \sum_{m=1}^{M} \hat{\theta_m}z_m\\    
&=  \bar{y} + X \sum_{m=1}^{M} \hat{\theta_m} v_m
\end{align*}
So we can just set $hat{\beta}^{pcr}(M) =  \sum_{m=1}^{M} \hat{\theta_m} v_m$ and we're done. 
Now it remains to show that  $\hat{\beta}^{pcr}(p) = \hat{\beta}^{ls}$.

\begin{align*}
    \hat{\beta}^{pcr}(p) &= \sum_{m=1}^{p} \hat{\theta_m} v_m\\
    &= V [\frac{z_1^T y}{z_1^T z_1}, \dots, \frac{z_p^T y}{z_p^T z_p}]^T\\
    &= V [\frac{z_1^T y}{d_1^2}, \dots, \frac{z_p^T y}{d_p^2}^T\\
    &= V D^{-2}[z_1^T y, \dots, z_p^T y]^T\\
    &= V D^{-2}[u_1^T d_1^T y, \dots, u_p^T d_p^T y]^T\\
    &= V D^{-2} D [u_1^T y, \dots, u_p^T y]^T\\
    &= V D^{-1} U^T y
\end{align*}

Where we made use of the SVD. Now: 

\begin{align*}
    \hat{\beta}^{ls} &= \left(X^T X\right)^{-1} X^T y \\
    &= \left(V D U^T U D V^T\right)^{-1} U D V^T y\\
    &= \left(V D D V^T\right)^{-1} V D U^T y\\
    &= D^{-2}\left(V V^T\right)^{-1} V D U^T y\\
    &= D^{-2} V D U^T y\\
    &= V D^{-1} U^T y
\end{align*}

Using the orthonomality of $U$ and $V$.

\subsection*{3.14 Show that in the orthogonal case, partial least squares stops after m = 1 steps.}

Assume $X$ is such that each column has mean zero, unit variance, and the colums are orthogonal.
let $z = \sum_i \left(x_i^T y\right) x_i$ and $\hat{\theta} = \frac{z^T y} {z^T z}$.

In this case 
\begin{align*}
    z^T z &= \sum_i \sum_j \left(x_i^T y\right)\left(x_j^T y\right) x_i^T x_j\\
    &= \sum_i \left(x_i^T y\right)^2 \left(x_i^T x_i\right)\\
    &= \sum_i \left(x_i^T y\right)^2\\
\end{align*}
Similarly $z^T y = \sum_i \left(x_i^T y\right)^2$. Now let $x_j^{(1)} = x_j - \frac{z^T x_j}{z^T z}$
Then let:
$$ x_j^{(1)} = x_j - \frac{z^T x_j}{ z^T z}z$$
In the next iteration, we have:
\begin{align*}
    \langle x_j^{(1)}, y\rangle &= x_j^T y - \frac{\left(x_j^T y\right) \left(x_j^T x_j\right)}{ \sum_i \left(x_i^T y\right)^2}z^T y\\
     &= x_j^T y - \frac{x_j^T y }{ \sum_i \left(x_i^T y\right)^2 }\sum_i \left(x_i^T y\right)^2\\
    &= x_j^T y - x_j^T y\\
    &= 0
\end{align*}

So algorithm 3.3 (page 81 in my edition) terminates after 1 step.






\subsection*{3.19 Show that $\norm{\hat{\beta}^{ridge}}$ increases as the tuning parameter $\lambda \rightarrow 0$. Does the same property hold for the Lasso and PLS?}

Throughout this question I use regression without intercept as the intercept is not included in the authors formulation of the penalty in the Lagrangian form.

a) Ridge\\
For data $X$ with mean 0 and unit variance, we have

$$\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^T y$$

Then:
\begin{align*}
     \norm{\hat{\beta}^{ridge}}^2 &= \left((X^TX + \lambda I)^{-1}X^T y\right)^T (X^TX + \lambda I)^{-1}X^T y \\
     &= \left(V (D^TD + \lambda I)^{-1}D U^T y\right)^T V(D^T D + \lambda I)^{-1} D U^T y \\
     &= y^T U D^2 (D^T D + \lambda I)^{-2} U^T y
\end{align*}
Using the SVD, the commutativity of $D$ and $V$, and the orthonormality of $V$.
$D^T D + \lambda I$ is diagonal, and so $D^2 (D^T D + \lambda I)^{-2}$ is too with entries $\frac{d_j^2}{d_j^2 + \lambda}$ on the diagonal.
Let $z_j = (U^T y)_j$
Then we see:
\begin{align*}
\norm{\hat{\beta}^{ridge}}^2 &= \sum_i \sum_j \frac{d_j^2 z_i^T z_j}{d_j^2 + \lambda}\\
&= \sum_j \frac{d_j^2 z_j^2}{d_j^2 + \lambda}\\
\end{align*} 

All terms are non-negative, and increase with decreasing $\lambda$ so $\hat{\beta}^{ridge}$ must too.
Recall that we can view the ridge regression estimate as the solution to

$$\hat{\beta}^{ridge} = argmin_\beta (y - X\beta)^T (y - X \beta) + \lambda \norm{\beta}_2^2$$

b) Lasso\\

Under a similar formulation, we have 
$$\hat{\beta}^{lasso} = argmin_\beta (y - X\beta)^T (y - X \beta) + \lambda \norm{\beta}_1$$

In a similar way one can see that if $\lambda$ is sufficiently large, it dominates this expression (for fixed $X$ and $y$), and so $\hat{\beta}^{lasso}$ will decrease in norm with increasing $\lambda$.

c) PLS\\

?






\subsection*{3.23. Please refer to the book. LAR}
a)Correlation with residuals remains constant in absolute value.\\

\begin{align*}
    \frac{1}{N} X^T \left(y - u(\alpha)\right) &= \frac{1}{N}\left(X^T y - \alpha X^T X\hat{\beta}\right)\\
    &= \frac{1}{N}\left(X^T y - \alpha X^T X \left(X^T X\right)^{-1} X^T y)\right)\\
    &= \frac{1}{N}\left(X^T y - \alpha X^T y)\right)\\
    &= \frac{1-\alpha}{N}X^T y\\
    \text{Thus }\lvert\frac{1}{N} X^T \left(y - u(\alpha)\right) \rvert &= (1-\alpha) [\lambda,\dots,\lambda]^T
\end{align*}
Which is exactly the required result in vector notation.

b)Explicit form of correlation\\

The question as stated ignores the need for an absolute value, so we shall assume that $\langle x_j, y - u(\alpha)\rangle \geq 0$ for every $j$, else replace $x_j$ by $-x_j$ in our data.

Correlations are given by covariance divided by the product of the standard deviations. as everything (data and response) is assumed to be standardised, we have:

\begin{align*}
    \left(y - u(\alpha))^T (y - u(\alpha)\right) &= 
    y^T y - 2 y^T u(\alpha) + u(\alpha)^T u(\alpha)\\
    &= y^T y - 2\alpha y^T X\hat{\beta} + \alpha^2 \hat{\beta}^T X^T X\hat{\beta}\\
    &= y^T y - 2\alpha y^T X\hat{\beta} + \alpha^2 \hat{\beta}^T X^T X \left(X^TX\right)^{-1} X^T y\\
    &= y^T y - 2\alpha y^T X\hat{\beta} + \alpha^2 \hat{\beta}^T X^T y\\
    &= y^T y + \alpha( \alpha - 2) y^T X\hat{\beta}\\
\end{align*}

Setting $\alpha = 1$ we get $RSS = y^T y - y^T X\hat{\beta}$.
Thus: $$\left(y - u(\alpha))^T (y - u(\alpha)\right) =  y^T y + \alpha( \alpha - 2) (y^T y - RSS)$$

And so:
\begin{align*}
\left(y - u(\alpha))^T (y - u(\alpha)\right) &=  (1 - \alpha)^2 y^T y + \alpha(2 - \alpha) RSS \\
&=  N (1 - \alpha)^2  + \alpha(2 - \alpha) RSS 
\end{align*} 

\begin{align*}
    Corr\left(x_j, y - u(\alpha)\right) &= \frac{\langle x_j, y - u(\alpha)\rangle / N}{\sqrt{\langle x_j, x_j \rangle / N}{\sqrt{\langle y - u(\alpha), y - u(\alpha) \rangle / N}}}\\
    &= \frac{\lambda (1 - \alpha)}{1\cdot{\sqrt{(1 - \alpha)^2  + \alpha(2 - \alpha) RSS / N} }}\\
    &= \frac{(1 - \alpha)}{{\sqrt{(1 - \alpha)^2  + \alpha(2 - \alpha) RSS / N} }} \lambda \\
\end{align*}
As required.

c) Show the LAR algorithm keeps correlations tied and monotonically decreasing.\\

Part a) showed that the LAR algorithm keeps correlations tied ad this is exactly the step taken for an active set of variables in the $kth$ step.
Part b) shows that correlations are monotonically decreasing with $\alpha$ as numerator falls  faster than the numerator for $\alpha$ in $[0,1]$\footnote{One can and should compute the gradient and check that it is negative on paper.}. In particular $\lambda(0) = \lambda$ and $\lambda(1) = 0$.

 





\subsection*{3.24 LAR directions}

Let $A_k$ be the active set of variables at the beginning of the $k$th step, and $\beta_{A_k}$ be the coefficient vector for these variables at this step. There are $k-1$ non-zero values of $\beta$ and one zero - the latest variable to enter the active set.
The current residual is denoted by $r_k = y - X_{A_k} \beta_{A_k}$ where $X_{A_k}$ is the subset of the data $X$ consisting of only the variables in $A_k$.
The direction for this step is 
$$\delta_k = \left(X_{A_k}^T X_{A_k}\right)^{-1} X_{A_k}^T r_k $$
Let $u_k = X_{A_k} \delta_k$ be the LAR direction vector.
Recall that the cosine of the angle between vectors $u$ and $v$ is $\frac{u^T v}{\norm{v}\norm{u}}$.
The columns $X_{A_k}$ have unit norm.
Thus the angle between $u_k$ and $x_j$ is $\frac{x_j^T u_k}{\norm{u_k}}$, and the denominator is constant given $u_k$.
That is to say, the angle between $u_k$ and the predictors in $A_k$ is constant if and only if $x_j^T u_k$ is.

\begin{align*}
    X_{A_k}^T u_k &= X_{A_k}^T X_{A_k}\delta_k\\
    &= X_{A_k}^T X_{A_k} \left(X_{A_k}^T X_{A_k}\right)^{-1} X_{A_k} r_k\\
    &= X_{A_k}^T r_k\\
\end{align*}
 The last term the correlation of each predictor with the residual, and in the LAR algorithm (and from 3.23) we know that all predictors in $A_k$ have the same correlation with the residual.
 Thus all the $x_j^T u_k$ are equal as required.

\subsection*{3.25 LAR Lookahead}

We use the notation from 3.24. $\beta_{A_k}(\alpha) = \beta_{A_k} + \alpha \delta_k$
$$r_k(\alpha) = \left(I - \alpha \delta_k\right) r_k$$
For any variable $x_i$ say, we have the correlation to $r_k(\alpha)$ defined as:
\begin{align*}
    c_i(\alpha) &= x_i^T r_k(\alpha)\\
    &= \left(x_i^T - \alpha x_i^T \delta_k\right) r_k
\end{align*}

For $i$ in the active set and some $j$ not in the active set, $x_j$ can only be added if these $c_i(\alpha)$ and $c_j(\alpha)$ are equal in absolute value. 
There are finitely many $j$ not in the active set, and for every $i$ in the active set, the correlations are the same so we can fix $i$.
There are two cases (depending on signs):
\begin{align*}
\alpha_j &= \frac{(x_i - x_j)^T r_k}{(x_i - x_j)^T\delta_k r_k}\\
\alpha_j &= \frac{(x_i + x_j)^T r_k}{(x_i + x_j)^T\delta_k r_k}
\end{align*}

We can compute all such $\alpha_j$, and then solve:
$$ argmax_{j \notin A_k} \abs{x_j^T r_k(\alpha_j)} $$
The result will be the next predictor to be included, with the corresponding $\alpha_j$ the value of $\alpha$ at which this predictor will be added.






\subsection*{3.29 Please see the book. Ridge regression with duplicate variables}

Setup: The data $X$ has identical columns $x$.

Let $X = [x, \dots, x]$ say, for $x$ some column vector, where $X$ has dimension $n \times m$.
The ridge regression beta is given by 
$$\left(X^T X + \lambda I\right)^{-1} X^T y$$
Note $X^T y = [x^T y, \dots, x^T y]^T$
$X^T X$ is a constant matrix with every value $x^T x$.
 
Let $M = X^T X / (x^T x)$ be the matrix of ones.
 
Looking for an inverse of the form $s M + \lambda^{-1} I$ yields 
$$s = \frac {-x^T x}{\lambda(\lambda + m x^T x)}$$
Where one uses that $M^T M = m M$.
 We have $$(sM + \lambda^{-1} I) [x^T y, \dots, x^T y]^T = [c, \dots, c]^T$$
 Where 
 \begin{align*}
     c &= \frac {-m \cdot x^T x\cdot x^T y}{\lambda (\lambda + m\cdot x^T x)}  + \frac{x^T y}{\lambda}\\
     &= \frac {(\lambda + m\cdot x^T x) x^T y -m \cdot x^T x\cdot x^T y}{\lambda (\lambda + m\cdot x^T x)} \\
     &= \frac {\lambda x^T y}{\lambda (\lambda + m\cdot x^T x)} \\
     &= \frac { x^T y}{ \lambda + m\cdot x^T x} 
 \end{align*}
 
 


\subsection*{3.30 Consider the elastic net optimisation problem:
$$ min_\beta \norm{y - X \beta}_2^2 + \lambda \left[ \alpha \norm{\beta}_2^2 + (1-\alpha) \norm{\beta}_1\right]$$
Show how one can turn this into a lasso problem using an augmented version of $X$ and $y$}

Let $X_2 = [X, \sqrt{\alpha \lambda}  I_p]^T$ where $X$ is $n \times p$ and we assume $X$ has been standardised while $y$ has mean $0$ (this is an easy augmentation to remove the constant in regression).
Let $y_2 = [y, 0]$ where we have augmented $y$ with $p$ additional zeros. This is the setup in 3.12 except here we use $\alpha \lambda$ in place of $\lambda$
\begin{align*}
    \norm{y_2 - X_2 \beta}_2^2 &= \norm{\left[y - X \beta, \sqrt{\alpha \lambda} \beta \right]^T}_2^2\\
    &= \norm{y - X \beta}_2^2 + \norm{\sqrt{\alpha \lambda} \beta}_2^2\\
    &= \norm{y - X \beta}_2^2 + \alpha \lambda \norm{\beta}_2^2\\
\end{align*}

With these data, the original problem becomes a lasso optimisation problem with parameter $\lambda (1-\alpha)$ :
$$min_\beta \norm{y_2 - X_2 \beta}_2^2 + \lambda (1-\alpha) \norm{\beta}_1$$